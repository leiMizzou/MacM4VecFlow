#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import time
import psycopg2
import numpy as np
import argparse
import socket
import json
import pickle
import logging
from psycopg2.extras import execute_values
from psycopg2.pool import ThreadedConnectionPool
from typing import List, Dict, Tuple, Any, Optional, Generator
from multiprocessing import Pool, cpu_count, current_process
from itertools import chain
from contextlib import contextmanager
import gc
import queue
from threading import Thread, Event, RLock
import threading
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import re
import math
import random
import psutil
import zmq  # æ·»åŠ ZeroMQç”¨äºèŠ‚ç‚¹é—´é€šä¿¡

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ç¦ç”¨è¿›åº¦æ¡
os.environ["TRANSFORMERS_NO_PROGRESS_BAR"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["TQDM_DISABLE"] = "1"  # ç¦ç”¨æ‰€æœ‰ tqdm è¿›åº¦æ¡

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆ†å¸ƒå¼è§’è‰²å®šä¹‰
COORDINATOR = 'coordinator'  # åè°ƒèŠ‚ç‚¹
WORKER = 'worker'  # å·¥ä½œèŠ‚ç‚¹

# M4ç³»ç»Ÿä¼˜åŒ–å‡½æ•°ä¿æŒä¸å˜
def optimize_system_for_m4():
    """Optimize system settings for M4 chip"""
    import platform
    
    # Only apply these optimizations on macOS with Apple Silicon
    if platform.system() != 'Darwin' or 'arm64' not in platform.machine():
        print("âš ï¸ é Apple Silicon Macï¼Œè·³è¿‡ç³»ç»Ÿä¼˜åŒ–")
        return
    
    print("ğŸ”§ åº”ç”¨ M4 ç³»ç»Ÿä¼˜åŒ–...")
    
    # 1. Set process priority (nice) if running with proper permissions
    try:
        os.nice(-10)  # Higher priority (lower nice value)
        print("  âœ“ è¿›ç¨‹ä¼˜å…ˆçº§å·²æé«˜")
    except:
        print("  âš ï¸ æ— æ³•è®¾ç½®è¿›ç¨‹ä¼˜å…ˆçº§")
    
    # 2. Optimize NumPy configuration
    try:
        # Use Apple Accelerate framework for NumPy
        os.environ["VECLIB_MAXIMUM_THREADS"] = str(os.cpu_count())
        print(f"  âœ“ NumPy é…ç½®ä¼˜åŒ–ï¼Œä½¿ç”¨ Accelerate æ¡†æ¶ï¼Œæœ€å¤§çº¿ç¨‹: {os.cpu_count()}")
    except:
        pass
    
    # 3. Memory allocation tuning
    try:
        os.environ["PYTHONMALLOC"] = "malloc"
        os.environ["MALLOC_TRIM_THRESHOLD_"] = "131072"  # 128KB
        print("  âœ“ å†…å­˜åˆ†é…å™¨å·²ä¼˜åŒ–")
    except:
        pass
    
    # 4. Configure Python GC for high-performance computing
    try:
        import gc
        gc.set_threshold(100000, 5, 5)  # Less frequent collections
        print("  âœ“ åƒåœ¾å›æ”¶å™¨å·²ä¼˜åŒ–")
    except:
        pass
        
    print("âœ… ç³»ç»Ÿä¼˜åŒ–å®Œæˆ")

# MPSä¼˜åŒ–å‡½æ•°ä¿æŒä¸å˜
def enhance_mps_performance():
    """å¢å¼ºM4èŠ¯ç‰‡MPSçš„æ€§èƒ½ä¼˜åŒ–"""
    import torch
    if not torch.backends.mps.is_available():
        print("âš ï¸ MPSä¸å¯ç”¨ï¼Œè·³è¿‡MPSä¼˜åŒ–")
        return False
        
    print("ğŸš€ æ­£åœ¨åº”ç”¨å¢å¼ºçš„MPSä¼˜åŒ–...")
    
    # 1. è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ–MPS
    os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"  # ä¸»åŠ¨å†…å­˜ç®¡ç†
    os.environ["PYTORCH_MPS_ALLOCATOR_POLICY"] = "1"        # æ›´ä¼˜çš„åˆ†é…ç­–ç•¥
    
    # 2. å¯ç”¨å¼‚æ­¥MPSæ‰§è¡Œ
    if hasattr(torch.mps, 'enable_async'):
        torch.mps.enable_async(True)
        print("  âœ“ å¯ç”¨MPSå¼‚æ­¥æ‰§è¡Œ")
    elif hasattr(torch.mps, 'set_async_execution'):
        torch.mps.set_async_execution(True)
        print("  âœ“ å¯ç”¨MPSå¼‚æ­¥æ‰§è¡Œ")
    
    # 3. é…ç½®Metalç¼–è¯‘å™¨ä¼˜åŒ–
    os.environ["MTL_SHADER_VALIDATION"] = "0"    # ç¦ç”¨shaderéªŒè¯æé«˜æ€§èƒ½
    os.environ["MTL_SHADER_CACHE_MODE"] = "11"   # ä¼˜åŒ–shaderç¼“å­˜
    
    # 4. é¢„çƒ­MPSè®¾å¤‡
    try:
        # åˆ›å»ºéšæœºæ•°æ®æ¥é¢„çƒ­å›¾å½¢ç®¡é“
        for size in [128, 384, 768, 1024]:
            dummy = torch.rand(size, size, device="mps")
            _ = dummy @ dummy.T  # çŸ©é˜µä¹˜æ³•é¢„çƒ­
            del dummy
            
        # å¯¹äºåµŒå…¥æ¨¡å‹çš„å…³é”®æ“ä½œé¢„çƒ­
        seq_len, dim = 128, 384
        q = torch.rand(1, seq_len, dim, device="mps")
        k = torch.rand(1, seq_len, dim, device="mps")
        v = torch.rand(1, seq_len, dim, device="mps")
        
        # é¢„çƒ­æ³¨æ„åŠ›è®¡ç®—
        qk = torch.matmul(q, k.transpose(1, 2))
        qk = qk / math.sqrt(dim)
        attn = torch.nn.functional.softmax(qk, dim=-1)
        _ = torch.matmul(attn, v)
        
        # æ¸…ç†
        del q, k, v, qk, attn
        torch.mps.empty_cache()
        
        print("  âœ“ MPSå›¾å½¢ç®¡é“é¢„çƒ­å®Œæˆ")
    except Exception as e:
        print(f"  âš ï¸ MPSé¢„çƒ­è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    # 5. é…ç½®MPSå†…å­˜ä¼˜åŒ–
    try:
        # ä¸»åŠ¨é‡Šæ”¾å†…å­˜å¹¶å°è¯•ä¿æŒè¾ƒä½çš„å†…å­˜å ç”¨
        torch.mps.empty_cache()
        
        # å»ºè®®ä½¿ç”¨çš„æœ€å¤§å†…å­˜å¤§å°
        # è¿™éœ€è¦PyTorchæ”¯æŒï¼Œæ¨¡æ‹Ÿç‰ˆæœ¬
        available_memory = psutil.virtual_memory().available
        suggested_max = min(int(available_memory * 0.7), 8 * 1024 * 1024 * 1024)  # æœ€å¤š8GB
        
        print(f"  âœ“ MPSå†…å­˜ä¼˜åŒ–é…ç½®å®Œæˆï¼Œå»ºè®®æœ€å¤§ä½¿ç”¨: {suggested_max/(1024*1024*1024):.1f}GB")
        
        # è®¾ç½®è‡ªå®šä¹‰ç¼“å­˜é‡Šæ”¾ç›‘è§†å™¨
        class MPSMemoryMonitor:
            def __init__(self, threshold_mb=2000):
                self.threshold_mb = threshold_mb
                self.last_check = time.time()
                self.check_interval = 5  # ç§’
                
            def check(self):
                current_time = time.time()
                if current_time - self.last_check < self.check_interval:
                    return
                
                self.last_check = current_time
                try:
                    # è·å–MPSåˆ†é…çš„å†…å­˜
                    allocated = torch.mps.current_allocated_memory() / (1024 * 1024)
                    if allocated > self.threshold_mb:
                        print(f"  âš ï¸ MPSå†…å­˜ä½¿ç”¨è¾ƒé«˜: {allocated:.0f}MBï¼Œæ‰§è¡Œç¼“å­˜æ¸…ç†")
                        torch.mps.empty_cache()
                except:
                    pass
        
        # åˆ›å»ºç›‘è§†å™¨
        global mps_memory_monitor
        mps_memory_monitor = MPSMemoryMonitor(threshold_mb=4000)  # 4GBé˜ˆå€¼
        print("  âœ“ MPSå†…å­˜ç›‘è§†å™¨å·²å¯åŠ¨")
    except Exception as e:
        print(f"  âš ï¸ é…ç½®MPSå†…å­˜ä¼˜åŒ–æ—¶å‡ºé”™: {e}")
    
    return True

# åˆå§‹åŒ–è®¾å¤‡
import torch
if torch.backends.mps.is_available():
    device = "mps"
    print(f"âœ… ä½¿ç”¨ Apple MPS (M4 GPU) åŠ é€Ÿ")
    # åº”ç”¨ MPS å¢å¼ºä¼˜åŒ–
    enhance_mps_performance()
    # é…ç½® PyTorch çº¿ç¨‹
    torch.set_num_threads(os.cpu_count() // 2)
    print(f"âœ“ PyTorch çº¿ç¨‹é…ç½®: {os.cpu_count() // 2}")
else:
    device = "cpu"
    print("âš ï¸ MPS ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU è®¡ç®—")

# å†…å­˜æ¸…ç†å‡½æ•°ä¿æŒä¸å˜
def clean_memory(force_gc=False):
    """Efficiently clean up memory"""
    import torch
    if device == "mps":
        # Synchronize MPS stream before cleaning
        if hasattr(torch.mps, 'synchronize'):
            torch.mps.synchronize()
        torch.mps.empty_cache()
        
        # æ£€æŸ¥MPSå†…å­˜ç›‘è§†å™¨
        if 'mps_memory_monitor' in globals():
            mps_memory_monitor.check()
    
    if force_gc:
        # More aggressive cleanup
        for _ in range(2):  # Multiple collections can help with fragmentation
            gc.collect()

# ä¼˜åŒ–æµæ°´çº¿å¹¶å‘åº¦å‡½æ•°ä¿æŒä¸å˜
def optimize_pipeline_concurrency():
    """æ ¹æ®M4èŠ¯ç‰‡ç‰¹æ€§ä¼˜åŒ–æµæ°´çº¿å¹¶å‘åº¦"""
    import platform
    import multiprocessing as mp
    
    # æ£€æµ‹å¯ç”¨ç‰©ç†æ ¸å¿ƒæ•°
    physical_cores = os.cpu_count()
    
    # æ£€æµ‹æ˜¯å¦ä¸ºApple Silicon
    is_apple_silicon = (
        platform.system() == 'Darwin' and 
        platform.machine() == 'arm64'
    )
    
    # å¯¹M4èŠ¯ç‰‡ç‰¹åˆ«ä¼˜åŒ–
    if is_apple_silicon:
        # æ£€æµ‹å…·ä½“èŠ¯ç‰‡å‹å·
        try:
            import subprocess
            chip_info = subprocess.check_output(
                "sysctl -n machdep.cpu.brand_string", 
                shell=True
            ).decode().strip()
            
            is_m4 = "M4" in chip_info
            
            if is_m4:
                print(f"æ£€æµ‹åˆ°M4èŠ¯ç‰‡: {chip_info}")
                # M4èŠ¯ç‰‡ä¼˜åŒ–é…ç½®
                # 1. æ–‡æœ¬åˆ‡åˆ† - CPUå¯†é›†å‹ï¼Œä½†ä¸éœ€è¦å¤ªå¤šå†…å­˜
                num_chunk_processes = max(2, physical_cores - 2)
                
                # 2. åµŒå…¥è®¡ç®— - MPS (GPU) ç“¶é¢ˆä»»åŠ¡
                # M4 MPSæ”¯æŒå¼‚æ­¥æ‰§è¡Œä½†å¹¶è¡Œæ€§æœ‰é™
                # é€šå¸¸2-3ä¸ªè¿›ç¨‹èƒ½è¾¾åˆ°æœ€ä½³å¹³è¡¡
                num_embedding_processes = 2  
                
                # 3. é˜Ÿåˆ—å¤§å°ä¼˜åŒ–
                chunk_queue_size = 10000  # è¶³å¤Ÿå¤§çš„ç¼“å†²åŒº
                result_queue_size = 15000  # ç¨å¤§çš„ç»“æœç¼“å†²åŒº
                
                # 4. æ•°æ®åº“å¹¶å‘å†™å…¥è¿æ¥
                db_pool_size = min(physical_cores, 6)
                
                # 5. è®¾ç½®åµŒå…¥æ‰¹å¤„ç†å¤§å°
                embedding_batch_size = 1024  # M4é»˜è®¤ä¸ºè¾ƒå¤§æ‰¹æ¬¡
            else:
                # å…¶ä»–Apple SiliconèŠ¯ç‰‡
                print(f"æ£€æµ‹åˆ°å…¶ä»–Apple SiliconèŠ¯ç‰‡: {chip_info}")
                num_chunk_processes = max(2, physical_cores // 2)
                num_embedding_processes = min(3, physical_cores // 4) 
                chunk_queue_size = 8000
                result_queue_size = 10000
                db_pool_size = min(physical_cores // 2, 4)
                embedding_batch_size = 512
        except:
            # æ— æ³•æ£€æµ‹å…·ä½“å‹å·ï¼Œä½¿ç”¨é€šç”¨é…ç½®
            print("ä½¿ç”¨é€šç”¨Apple Siliconä¼˜åŒ–")
            num_chunk_processes = max(2, physical_cores // 2)
            num_embedding_processes = 2
            chunk_queue_size = 8000
            result_queue_size = 10000
            db_pool_size = 4
            embedding_batch_size = 512
    else:
        # éApple Siliconä¼˜åŒ–
        print("ä½¿ç”¨é€šç”¨ä¼˜åŒ–é…ç½®")
        num_chunk_processes = max(2, physical_cores // 2)
        num_embedding_processes = min(2, max(1, physical_cores // 4))
        chunk_queue_size = 5000
        result_queue_size = 8000
        db_pool_size = min(physical_cores // 2, 8)
        embedding_batch_size = 512
    
    # æ‰“å°ä¼˜åŒ–é…ç½®
    print(f"ğŸ“Š ä¼˜åŒ–é…ç½®:")
    print(f"  â†’ æ–‡æœ¬åˆ‡åˆ†è¿›ç¨‹æ•°: {num_chunk_processes}")
    print(f"  â†’ åµŒå…¥è®¡ç®—è¿›ç¨‹æ•°: {num_embedding_processes}")
    print(f"  â†’ æ•°æ®åº“è¿æ¥æ± å¤§å°: {db_pool_size}")
    print(f"  â†’ åµŒå…¥æ‰¹å¤„ç†å¤§å°: {embedding_batch_size}")
    
    return {
        'num_chunk_processes': num_chunk_processes,
        'num_embedding_processes': num_embedding_processes,
        'chunk_queue_size': chunk_queue_size,
        'result_queue_size': result_queue_size,
        'db_pool_size': db_pool_size,
        'embedding_batch_size': embedding_batch_size
    }

# åµŒå…¥ç¼“å­˜ä¼˜åŒ–ç±»ä¿æŒä¸å˜
class EmbeddingCache:
    """ä¸ºå¸¸è§æ–‡æœ¬ç‰‡æ®µæä¾›åµŒå…¥ç¼“å­˜ä»¥å‡å°‘è®¡ç®—"""
    
    def __init__(self, max_size=10000):
        self.cache = {}
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
        self.lock = threading.RLock()
    
    def get(self, text):
        """ä»ç¼“å­˜è·å–åµŒå…¥"""
        text_hash = hash(text)
        with self.lock:
            if text_hash in self.cache:
                self.hits += 1
                return self.cache[text_hash]
            self.misses += 1
            return None
    
    def put(self, text, embedding):
        """å‘ç¼“å­˜æ·»åŠ åµŒå…¥"""
        text_hash = hash(text)
        with self.lock:
            # å¦‚æœç¼“å­˜æ»¡ï¼Œç§»é™¤éšæœºé¡¹
            if len(self.cache) >= self.max_size:
                # éšæœºç§»é™¤10%çš„ç¼“å­˜ä»¥è…¾å‡ºç©ºé—´
                keys_to_remove = random.sample(
                    list(self.cache.keys()), 
                    k=max(1, int(self.max_size * 0.1))
                )
                for key in keys_to_remove:
                    del self.cache[key]
                    
            self.cache[text_hash] = embedding
    
    def stats(self):
        """è¿”å›ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            total = self.hits + self.misses
            hit_rate = self.hits / total if total > 0 else 0
            return {
                "size": len(self.cache),
                "max_size": self.max_size,
                "hits": self.hits,
                "misses": self.misses,
                "hit_rate": hit_rate,
            }
            
    def clear(self):
        """æ¸…é™¤ç¼“å­˜"""
        with self.lock:
            self.cache.clear()
            self.hits = 0
            self.misses = 0

# å¯¼å…¥ huggingface ç›¸å…³åº“
from transformers import logging as hf_logging
hf_logging.set_verbosity_error()

# å¯¼å…¥ SentenceTransformer å¹¶æŒ‡å®šä½¿ç”¨ MPS è®¾å¤‡
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer

# ä¼˜åŒ–ç‰ˆæœ¬çš„æ¨¡å‹åŠ è½½å‡½æ•°ä¿æŒä¸å˜
def load_optimized_model(model_name, device):
    """ä½¿ç”¨åŠç²¾åº¦åŠ è½½æ¨¡å‹ä»¥å‡å°‘å†…å­˜å ç”¨å¹¶æé«˜é€Ÿåº¦"""
    import torch
    
    print(f"æ­£åœ¨ä¼˜åŒ–åŠ è½½æ¨¡å‹åˆ° {device} è®¾å¤‡...")
    
    # ä½¿ç”¨fp16åŠ è½½æ¨¡å‹
    model = SentenceTransformer(model_name, device=device)
    
    # å¦‚æœè®¾å¤‡æ”¯æŒï¼Œè½¬æ¢ä¸ºåŠç²¾åº¦
    if device == "mps":
        # ç¡®ä¿MPSæ”¯æŒfp16
        if hasattr(torch.ops.mps, '_has_fp16_support') and torch.ops.mps._has_fp16_support():
            # å°†æ¨¡å‹è½¬æ¢ä¸ºfp16
            model.half()
            print("âœ… æ¨¡å‹å·²è½¬æ¢ä¸ºåŠç²¾åº¦(fp16)")
        
        # å¦‚æœæ”¯æŒç‰¹å®šçš„MPSå›¾åƒé›†ä¼˜åŒ–
        if hasattr(torch.mps, 'optimize_for'):
            # æŒ‡å®šä¸ºè½¬æ¢å™¨ä¼˜åŒ–
            torch.mps.optimize_for(model, category="transformer_inference")
            print("âœ… ä½¿ç”¨MPSå›¾ä¼˜åŒ–å™¨ä¼˜åŒ–")
    
    return model

# æ·»åŠ åˆ†å¸ƒå¼é€šä¿¡å·¥å…·ç±»
class DistributedComm:
    """åˆ†å¸ƒå¼èŠ‚ç‚¹é—´é€šä¿¡ç±»"""
    
    def __init__(self, role, coordinator_ip='localhost', coordinator_port=5555, worker_port=5556):
        self.role = role
        self.coordinator_ip = coordinator_ip
        self.coordinator_port = coordinator_port
        self.worker_port = worker_port
        self.context = zmq.Context()
        self.sockets = {}
        
        # åˆå§‹åŒ–é€šä¿¡
        self._init_communication()
        
    def _init_communication(self):
        """æ ¹æ®èŠ‚ç‚¹è§’è‰²åˆå§‹åŒ–é€šä¿¡å¥—æ¥å­—"""
        if self.role == COORDINATOR:
            # åè°ƒå™¨éœ€è¦ä¸€ä¸ªROUTERå¥—æ¥å­—æ¥æ¥æ”¶å·¥ä½œèŠ‚ç‚¹çš„è¿æ¥
            self.sockets['workers'] = self.context.socket(zmq.ROUTER)
            self.sockets['workers'].bind(f"tcp://*:{self.coordinator_port}")
            
            # åè°ƒå™¨è¿˜éœ€è¦ä¸€ä¸ªDEALERå¥—æ¥å­—å‘å·¥ä½œèŠ‚ç‚¹å‘é€ä»»åŠ¡
            self.sockets['tasks'] = self.context.socket(zmq.DEALER)
            self.sockets['tasks'].bind(f"tcp://*:{self.worker_port}")
            
            print(f"åè°ƒå™¨ç›‘å¬åœ¨ç«¯å£ {self.coordinator_port}(workers) å’Œ {self.worker_port}(tasks)")
        else:
            # å·¥ä½œèŠ‚ç‚¹éœ€è¦è¿æ¥åˆ°åè°ƒå™¨
            self.sockets['coordinator'] = self.context.socket(zmq.DEALER)
            # ä¸ºè¿™ä¸ªå·¥ä½œèŠ‚ç‚¹ç”Ÿæˆå”¯ä¸€æ ‡è¯†
            identity = f"worker-{socket.gethostname()}-{os.getpid()}"
            self.sockets['coordinator'].setsockopt_string(zmq.IDENTITY, identity)
            self.sockets['coordinator'].connect(f"tcp://{self.coordinator_ip}:{self.coordinator_port}")
            
            # å·¥ä½œèŠ‚ç‚¹è¿˜éœ€è¦ä¸€ä¸ªå¥—æ¥å­—æ¥æ¥æ”¶ä»»åŠ¡
            self.sockets['tasks'] = self.context.socket(zmq.DEALER)
            self.sockets['tasks'].connect(f"tcp://{self.coordinator_ip}:{self.worker_port}")
            
            print(f"å·¥ä½œèŠ‚ç‚¹ {identity} å·²è¿æ¥åˆ°åè°ƒå™¨ {self.coordinator_ip}")

    def send_message(self, socket_name, message):
        """å‘é€æ¶ˆæ¯åˆ°æŒ‡å®šå¥—æ¥å­—"""
        try:
            # Convert to JSON string
            json_data = json.dumps(message)
            #print(f"Sending message to {socket_name}: {json_data[:100]}...")  # Only print first 100 chars
            self.sockets[socket_name].send_string(json_data)
            return True
        except Exception as e:
            print(f"å‘é€æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            return False

    def receive_message(self, socket_name, timeout=1000):
        """ä»æŒ‡å®šå¥—æ¥å­—æ¥æ”¶æ¶ˆæ¯ï¼Œå¯è®¾ç½®è¶…æ—¶(æ¯«ç§’)"""
        try:
            # è®¾ç½®æ¥æ”¶è¶…æ—¶
            self.sockets[socket_name].setsockopt(zmq.RCVTIMEO, timeout)
            
            # Check if this is a ROUTER socket
            if self.role == COORDINATOR and socket_name == 'workers':
                # For ROUTER sockets, receive both identity and message parts
                identity = self.sockets[socket_name].recv_string()
                message = self.sockets[socket_name].recv_string()
                print(f"Received message from identity {identity}")
                
                if not message or message.strip() == "":
                    return None
                    
                return json.loads(message)
            else:
                # For non-ROUTER sockets, just receive the message
                message = self.sockets[socket_name].recv_string()
                
                if not message or message.strip() == "":
                    return None
                    
                return json.loads(message)
        except zmq.error.Again:
            # è¶…æ—¶ï¼Œæ²¡æœ‰æ”¶åˆ°æ¶ˆæ¯
            return None
        except Exception as e:
            print(f"æ¥æ”¶æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            if 'message' in locals():
                print(f"Raw message content (first 100 chars): {repr(message[:100] if message else 'None')}")
            return None
    def close(self):
        """å…³é—­æ‰€æœ‰å¥—æ¥å­—"""
        for socket in self.sockets.values():
            socket.close()
        self.context.term()
        print("é€šä¿¡èµ„æºå·²é‡Šæ”¾")

# ä¿®æ”¹å‘é‡å¤„ç†å™¨ç±»ä»¥æ”¯æŒåˆ†å¸ƒå¼
class PgVectorProcessor:
    def __init__(self, db_config: Dict[str, Any]):
        self.db_config = db_config
        self.conn = None
        self.pool = None
        self.vector_dim = 384
        self.chunk_size = 128  # è¾ƒå¤§çš„æ–‡æœ¬å—ä»¥å‡å°‘æ€»å—æ•°
        self.batch_size = 1024  # å¢åŠ æ‰¹å¤„ç†å¤§å°ä»¥æé«˜ M4 æ€§èƒ½
        
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° {device} è®¾å¤‡...")
        
        # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ - sentence-transformers/all-MiniLM-L6-v2
        model_name = 'sentence-transformers/all-MiniLM-L6-v2'
        
        try:
            # ä½¿ç”¨ä¼˜åŒ–çš„æ¨¡å‹åŠ è½½
            self.model = load_optimized_model(model_name, device)
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
            
            # å°è¯•ä½¿ç”¨ MPS åŠ é€Ÿ
            if device == "mps":
                print(f"âœ… æ¨¡å‹æˆåŠŸåŠ è½½åˆ° MPS è®¾å¤‡")
                
                # å°è¯•é¢„çƒ­æ¨¡å‹
                dummy_texts = ["This is a warm up text to initialize the model pipeline."] * 4
                _ = self.model.encode(dummy_texts, convert_to_numpy=True)
                print(f"âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½åˆ° {device} å¤±è´¥: {e}ï¼Œå›é€€åˆ° CPU")
            try:
                self.model = SentenceTransformer(model_name)
                print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹åˆ° CPU: {model_name}")
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹åŠ è½½å®Œå…¨å¤±è´¥: {e}")
                raise
        
        # åŠ è½½å¯¹åº”çš„ tokenizer
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            print(f"âœ… æˆåŠŸåŠ è½½å¯¹åº” tokenizer")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ tokenizer å¤±è´¥: {e}ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
            self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
        
        # å¯ç”¨ MPS å›¾æ•è· (å¦‚æœå¯ç”¨)
        if device == "mps" and hasattr(torch.mps, 'enable_graph_capture'):
            try:
                torch.mps.enable_graph_capture()
                print(f"âœ… å¯ç”¨ MPS å›¾æ•è·ä»¥æé«˜æ€§èƒ½")
            except:
                pass
        
        print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œä½¿ç”¨çš„è®¾å¤‡: {self.model.device}")
        
        # åˆå§‹åŒ–è¿æ¥æ± 
        self.init_connection_pool()
        
        # åˆ›å»ºåµŒå…¥ç¼“å­˜
        self.embedding_cache = EmbeddingCache(max_size=50000)  # ç¼“å­˜æœ€å¤š50,000ä¸ªåµŒå…¥

    def init_connection_pool(self, min_conn=2, max_conn=10):
        """Initialize a connection pool for better database performance"""
        try:
            self.pool = ThreadedConnectionPool(
                minconn=min_conn,
                maxconn=max_conn,
                **self.db_config
            )
            logger.info(f"æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–æˆåŠŸ (min={min_conn}, max={max_conn})")
            return True
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def process_text(self, text: str) -> List[str]:
        """Improved text chunking with better semantic boundaries"""
        if not text or text.strip() == "":
            return []
        
        # 1. æ¸…ç†å’Œè§„èŒƒåŒ–æ–‡æœ¬
        text = text.replace('\r', ' ').replace('\t', ' ')
        
        # 2. å°è¯•ä½¿ç”¨è‡ªç„¶å¥å­è¾¹ç•Œè¿›è¡Œåˆ†å—
        
        # æŒ‰å¥å­è¾¹ç•Œåˆ†å‰²ï¼ˆåŸåˆ™ä¸Šï¼‰
        # æ­¤æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¥å­ç»“å°¾ - å¥å·ã€é—®å·ã€æ„Ÿå¹å·åè·Ÿç©ºæ ¼
        sentence_endings = re.compile(r'(?<=[.!?])\s+')
        
        # åˆ†å‰²ä¸ºå¥å­
        sentences = sentence_endings.split(text)
        
        # è¿‡æ»¤ç©ºå¥å­
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # 3. æ ¹æ®ä»¤ç‰Œè®¡æ•°å°†å¥å­åˆ†ç»„æˆå—
        chunks = []
        current_chunk = []
        current_length = 0
        target_chunk_size = self.chunk_size
        overlap_tokens = 20  # è¾ƒå°çš„é‡å ä»¥ä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§
        
        for sentence in sentences:
            # å¯¹å¥å­è¿›è¡Œåˆ†è¯
            sentence_tokens = self.tokenizer.encode(sentence)
            sentence_length = len(sentence_tokens)
            
            # é€šè¿‡åˆ†å‰²æ¥è·³è¿‡æé•¿çš„å•å¥
            if sentence_length > target_chunk_size:
                if current_chunk:
                    # ç¼–ç å’Œè§£ç ä»¥è·å¾—é€‚å½“çš„æ ‡è®°è¾¹ç•Œ
                    chunk_text = self.tokenizer.decode(
                        self.tokenizer.encode(' '.join(current_chunk))
                    )
                    chunks.append(chunk_text)
                    current_chunk = []
                    current_length = 0
                
                # å°†é•¿å¥å¤„ç†æˆå…·æœ‰é‡å çš„å¤šä¸ªå—
                for i in range(0, sentence_length, target_chunk_size - overlap_tokens):
                    sub_tokens = sentence_tokens[i:i + target_chunk_size]
                    sub_text = self.tokenizer.decode(sub_tokens)
                    chunks.append(sub_text)
                
                continue
            
            # æ£€æŸ¥æ·»åŠ æ­¤å¥æ˜¯å¦ä¼šè¶…å‡ºå—å¤§å°
            if current_length + sentence_length > target_chunk_size:
                # å½“å‰å—å·²æ»¡ï¼Œå°†å…¶æ·»åŠ åˆ°å—ä¸­å¹¶å¼€å§‹æ–°å—
                if current_chunk:
                    chunk_text = self.tokenizer.decode(
                        self.tokenizer.encode(' '.join(current_chunk))
                    )
                    chunks.append(chunk_text)
                
                # ç”¨æ­¤å¥å¼€å§‹æ–°å—
                current_chunk = [sentence]
                current_length = sentence_length
            else:
                # æ·»åŠ åˆ°å½“å‰å—
                current_chunk.append(sentence)
                current_length += sentence_length
        
        # æ·»åŠ å‰©ä½™çš„å—
        if current_chunk:
            chunk_text = self.tokenizer.decode(
                self.tokenizer.encode(' '.join(current_chunk))
            )
            chunks.append(chunk_text)
        
        # ç¡®ä¿å³ä½¿å¯¹äºå¾ˆçŸ­çš„æ–‡æœ¬ä¹Ÿè‡³å°‘æœ‰ä¸€ä¸ªå—
        if not chunks and text.strip():
            chunks.append(text.strip())
        
        return chunks

    def connect(self) -> bool:
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        try:
            if self.conn is not None and not self.conn.closed:
                logger.info("å·²å­˜åœ¨æœ‰æ•ˆçš„æ•°æ®åº“è¿æ¥")
                return True
                
            self.conn = psycopg2.connect(**self.db_config)
            logger.info("æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False

    def close(self) -> None:
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        try:
            if self.conn and not self.conn.closed:
                self.conn.close()
                logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")
            
            # å…³é—­è¿æ¥æ± 
            if self.pool:
                self.pool.closeall()
                logger.info("æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")

    @contextmanager
    def get_connection(self):
        """ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç®¡ç†æ•°æ®åº“è¿æ¥"""
        connection = None
        connection_created = False
        
        try:
            # å°è¯•ä»è¿æ¥æ± è·å–è¿æ¥
            if self.pool:
                connection = self.pool.getconn()
                yield connection
            else:
                # å›é€€åˆ°å¸¸è§„è¿æ¥
                if not self.conn or self.conn.closed:
                    if not self.connect():
                        raise RuntimeError("æ— æ³•å»ºç«‹æ•°æ®åº“è¿æ¥")
                    connection_created = True
                
                yield self.conn
        finally:
            # å½’è¿˜è¿æ¥åˆ°æ± 
            if connection and self.pool:
                self.pool.putconn(connection)
            elif connection_created:
                self.close()

    def check_vector_extension(self) -> bool:
        """æ£€æŸ¥ PostgreSQL vector æ‰©å±•æ˜¯å¦å·²å®‰è£…"""
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT * FROM pg_extension WHERE extname = 'vector'")
                    if cur.fetchone() is None:
                        logger.error("PostgreSQL 'vector' æ‰©å±•æœªå®‰è£…")
                        return False
                    logger.info("PostgreSQL 'vector' æ‰©å±•å·²å®‰è£…")
                    return True
        except Exception as e:
            logger.error(f"æ£€æŸ¥ vector æ‰©å±•æ—¶å‡ºé”™: {e}")
            return False

    def setup_vector_db(self, table_name: str) -> bool:
        """è®¾ç½®å‘é‡æ•°æ®åº“è¡¨åŠç´¢å¼•"""
        if not self.check_vector_extension():
            logger.error("æœªæ‰¾åˆ°å¿…è¦çš„ vector æ‰©å±•ï¼Œæ— æ³•åˆ›å»ºå‘é‡è¡¨")
            return False
            
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cur:
                    # æ£€æŸ¥ schema æ˜¯å¦å­˜åœ¨
                    cur.execute("""
                    SELECT schema_name FROM information_schema.schemata 
                    WHERE schema_name = 'device'
                    """)
                    if cur.fetchone() is None:
                        logger.error("device schema ä¸å­˜åœ¨")
                        return False
                        
                    cur.execute(f"""
                    CREATE TABLE IF NOT EXISTS {table_name} (
                        id SERIAL PRIMARY KEY,
                        report_number TEXT NOT NULL,
                        chunk_id TEXT NOT NULL,
                        text_chunk TEXT NOT NULL,
                        embedding public.vector({self.vector_dim}),
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(chunk_id)
                    );
                    """)
                    cur.execute(f"""
                    CREATE INDEX IF NOT EXISTS {table_name}_embedding_idx 
                    ON {table_name} USING ivfflat (embedding public.vector_cosine_ops)
                    WITH (lists = 100);
                    """)
                    conn.commit()
                    logger.info(f"å‘é‡è¡¨ {table_name} åˆ›å»ºæˆåŠŸ")
                    return True
        except Exception as e:
            logger.error(f"è®¾ç½®å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return False

    def get_embeddings_batch_optimized(self, texts: List[str]) -> List[List[float]]:
        """ä¼˜åŒ–çš„æ‰¹é‡åµŒå…¥ç”Ÿæˆï¼Œè‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°"""
        if not texts:
            return []
            
        # è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_texts = [t for t in texts if t and t.strip()]
        if not valid_texts:
            return [[0.0] * self.vector_dim for _ in range(len(texts))]
        
        try:
            # è·Ÿè¸ªåŸå§‹ç´¢å¼•ä»¥ä¾¿å¯¹é½
            indices = [i for i, t in enumerate(texts) if t and t.strip()]
            
            # å‡†å¤‡è§„èŒƒåŒ–è¾“å…¥
            normalized_texts = []
            for t in valid_texts:
                # æ›´é«˜æ•ˆçš„æ–‡æœ¬è§„èŒƒåŒ–
                t = re.sub(r'\s+', ' ', t).strip()
                normalized_texts.append(t)
            
            # åŠ¨æ€æ‰¹å¤„ç†å¤§å° - åŸºäºæ–‡æœ¬é•¿åº¦å’Œå¯ç”¨å†…å­˜
            avg_token_len = sum(len(t.split()) for t in normalized_texts) / len(normalized_texts)
            
            # é’ˆå¯¹M4èŠ¯ç‰‡ä¼˜åŒ–çš„æ‰¹å¤„ç†å¤§å°
            if device == "mps":
                # æ ¹æ®å¹³å‡æ–‡æœ¬é•¿åº¦åŠ¨æ€è°ƒæ•´æ‰¹å¤§å°
                if avg_token_len < 50:
                    batch_size = 2048  # çŸ­æ–‡æœ¬ä½¿ç”¨æ›´å¤§æ‰¹æ¬¡
                elif avg_token_len < 100:
                    batch_size = 1024  # ä¸­ç­‰é•¿åº¦æ–‡æœ¬
                else:
                    batch_size = 512   # é•¿æ–‡æœ¬ä½¿ç”¨å°æ‰¹æ¬¡
            else:
                # CPUå¤„ç†ä½¿ç”¨è¾ƒå°æ‰¹æ¬¡é¿å…å†…å­˜é—®é¢˜
                batch_size = min(512, max(64, int(10000 / avg_token_len)))
                
            # ä½¿ç”¨å›ºå®šé•¿åº¦å¡«å……ä¼˜åŒ–
            # æŒ‰é•¿åº¦åˆ†ç»„ï¼Œç›¸ä¼¼é•¿åº¦æ–‡æœ¬ä¸€èµ·å¤„ç†å¯å‡å°‘å¡«å……å¼€é”€
            length_groups = {}
            for i, text in enumerate(normalized_texts):
                text_len = len(text.split())
                length_bucket = text_len // 20 * 20  # åˆ†ç»„ä¸º20è¯ä¸€ä¸ªæ¡¶
                if length_bucket not in length_groups:
                    length_groups[length_bucket] = []
                length_groups[length_bucket].append((i, text))
            
            # æŒ‰é•¿åº¦åˆ†ç»„å¤„ç†
            all_embeddings = [None] * len(normalized_texts)
            for length_bucket in sorted(length_groups.keys()):
                bucket_indices, bucket_texts = zip(*length_groups[length_bucket])
                
                # ä½¿ç”¨normalize_embeddings=Trueè‡ªåŠ¨å½’ä¸€åŒ–,æé«˜æ€§èƒ½
                # convert_to_tensor=Trueé¿å…CPU-GPUè½¬æ¢å¼€é”€
                bucket_embeddings = self.model.encode(
                    list(bucket_texts),
                    show_progress_bar=False,
                    batch_size=batch_size,
                    convert_to_tensor=True,
                    normalize_embeddings=True
                )
                
                # è½¬æ¢ä¸ºnumpyä»¥ä¾¿è¿›ä¸€æ­¥å¤„ç†(å¦‚æœå°šæœªè½¬æ¢)
                if hasattr(bucket_embeddings, 'cpu'):
                    bucket_embeddings = bucket_embeddings.cpu().numpy()
                    
                # å°†ç»„å†…åµŒå…¥æ”¾å›ç›¸åº”ä½ç½®
                for idx, emb in zip(bucket_indices, bucket_embeddings):
                    all_embeddings[idx] = emb
                    
            # æ„å»ºæœ€ç»ˆç»“æœ
            result = [[0.0] * self.vector_dim for _ in range(len(texts))]
            for orig_idx, emb_idx in enumerate(indices):
                # ç¡®ä¿æ˜¯åˆ—è¡¨æ ¼å¼
                emb = all_embeddings[orig_idx]
                emb_list = emb.tolist() if hasattr(emb, 'tolist') else list(emb)
                # å¤„ç†NaNå€¼
                if any(np.isnan(x) for x in emb_list):
                    emb_list = [0.0 if np.isnan(x) else x for x in emb_list]
                result[emb_idx] = emb_list
                
            # æ‰‹åŠ¨è§¦å‘MPSç¼“å­˜æ¸…ç†
            if device == "mps":
                torch.mps.empty_cache()
                
            return result
        except Exception as e:
            logger.error(f"æ‰¹é‡è·å–åµŒå…¥å‘é‡æ—¶å‡ºé”™: {e}")
            # å‡ºé”™æ—¶è¿”å›é›¶å‘é‡
            return [[0.0] * self.vector_dim for _ in range(len(texts))]
    
    # å‘åå…¼å®¹çš„API
    def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """ä¼˜åŒ–çš„æ‰¹é‡åµŒå…¥ç”Ÿæˆï¼Œå…¼å®¹æ€§æ›´å¥½"""
        # å…ˆæ£€æŸ¥ç¼“å­˜
        cached_results = []
        uncached_indices = []
        uncached_texts = []
        
        # è·Ÿè¸ªå“ªäº›æ–‡æœ¬éœ€è¦è®¡ç®—
        for i, text in enumerate(texts):
            if not text or not text.strip():
                cached_results.append([0.0] * self.vector_dim)
                continue
                
            cached_emb = self.embedding_cache.get(text)
            if cached_emb is not None:
                cached_results.append(cached_emb)
            else:
                cached_results.append(None)
                uncached_indices.append(i)
                uncached_texts.append(text)
        
        # å¦‚æœæ‰€æœ‰æ–‡æœ¬éƒ½åœ¨ç¼“å­˜ä¸­ï¼Œç›´æ¥è¿”å›
        if not uncached_texts:
            return cached_results
            
        # å¤„ç†æœªç¼“å­˜çš„åµŒå…¥
        uncached_embeddings = self.get_embeddings_batch_optimized(uncached_texts)
        
        # å°†æœªç¼“å­˜çš„åµŒå…¥æ·»åŠ åˆ°ç¼“å­˜
        for i, (idx, emb) in enumerate(zip(uncached_indices, uncached_embeddings)):
            if i < len(uncached_texts):
                self.embedding_cache.put(uncached_texts[i], emb)
                cached_results[idx] = emb
        
        return cached_results

    def store_vectors(self, table_name: str, data: List[Tuple]) -> bool:
        """ä½¿ç”¨ COPY å‘½ä»¤ä¼˜åŒ–å‘é‡å­˜å‚¨"""
        if not data:
            logger.warning("æ²¡æœ‰å‘é‡æ•°æ®éœ€è¦å­˜å‚¨")
            return True
            
        try:
            with self.get_connection() as conn:
                with conn.cursor() as cur:
                    # ä¼˜åŒ–æ•°æ®åº“å†™å…¥
                    
                    # 1. ä½¿ç”¨ COPY ä»£æ›¿ INSERT è¿›è¡Œæ‰¹é‡æ“ä½œ
                    from io import StringIO
                    
                    # åˆ›å»ºå…·æœ‰ç›¸åŒç»“æ„çš„ä¸´æ—¶è¡¨
                    temp_table = f"temp_{table_name}_{int(time.time())}"
                    cur.execute(f"""
                    CREATE TEMPORARY TABLE {temp_table} (
                        report_number TEXT,
                        chunk_id TEXT,
                        text_chunk TEXT,
                        embedding public.vector({self.vector_dim})
                    ) ON COMMIT DROP;
                    """)
                    
                    # å‡†å¤‡æ•°æ®è¿›è¡Œ COPY
                    buffer = StringIO()
                    for report_number, chunk_id, text_chunk, embedding in data:
                        # å°†åµŒå…¥æ ¼å¼åŒ–ä¸º PostgreSQL å‘é‡å­—ç¬¦ä¸²
                        embedding_str = f"[{','.join(str(x) for x in embedding)}]"
                        # è½¬ä¹‰åˆ†éš”ç¬¦å’Œå¼•å·
                        text = text_chunk.replace('\t', ' ').replace('\n', ' ').replace('\\', '\\\\')
                        buffer.write(f"{report_number}\t{chunk_id}\t{text}\t{embedding_str}\n")
                    
                    buffer.seek(0)
                    
                    # ä½¿ç”¨ COPY è¿›è¡Œæ›´å¿«çš„æ’å…¥
                    cur.copy_expert(f"COPY {temp_table} FROM STDIN", buffer)
                    
                    # ä»ä¸´æ—¶è¡¨æ’å…¥åˆ°ä¸»è¡¨ï¼Œå¤„ç†å†²çª
                    cur.execute(f"""
                    INSERT INTO {table_name} (report_number, chunk_id, text_chunk, embedding)
                    SELECT report_number, chunk_id, text_chunk, embedding 
                    FROM {temp_table}
                    ON CONFLICT (chunk_id) DO NOTHING;
                    """)
                    
                    conn.commit()
                return True
        except Exception as e:
            logger.error(f"å­˜å‚¨å‘é‡å¤±è´¥: {e}")
            # å¦‚æœä¼˜åŒ–ç‰ˆæœ¬å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•
            try:
                # å›é€€åˆ°ä½¿ç”¨ execute_values
                with self.get_connection() as conn:
                    with conn.cursor() as cur:
                        # ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡æ‰§è¡Œæ’å…¥æ“ä½œ
                        batch_size = 5000
                        for i in range(0, len(data), batch_size):
                            batch = data[i:i + batch_size]
                            execute_values(cur, f"""
                            INSERT INTO {table_name} 
                            (report_number, chunk_id, text_chunk, embedding)
                            VALUES %s
                            ON CONFLICT (chunk_id) DO NOTHING;
                            """, batch)
                            
                            # æäº¤å½“å‰æ‰¹æ¬¡ï¼Œå‡å°‘äº‹åŠ¡å¤§å°
                            conn.commit()
                return True
            except:
                return False

# æ–‡æœ¬åˆ‡åˆ†çš„å·¥ä½œå‡½æ•° - æå‰å®šä¹‰ä»¥ä¾›è°ƒç”¨
def chunk_process_worker(args: Tuple[List[Tuple[str, str]], dict]) -> List[Tuple[str, int, str]]:
    """å¤„ç†æ–‡æœ¬åˆ‡åˆ†çš„å·¥ä½œå‡½æ•°"""
    batch, db_config = args
    processor = PgVectorProcessor(db_config)
    results = []
    
    try:
        for report_number, text in batch:
            if not text:
                continue
            chunks = processor.process_text(text)
            for i, chunk in enumerate(chunks):
                if not chunk or chunk.strip() == "":
                    continue
                # åªè¿”å›å¿…è¦çš„æ•°æ®ï¼šæŠ¥å‘Šå·ã€å—ç´¢å¼•ã€æ–‡æœ¬å†…å®¹
                results.append((str(report_number), i+1, chunk))
                
                # é¢‘ç¹é‡Šæ”¾å†…å­˜
                if len(results) % 10000 == 0:
                    # å¼ºåˆ¶æ¸…ç†å†…å­˜
                    clean_memory(force_gc=False)
    except Exception as e:
        print(f"æ–‡æœ¬åˆ‡åˆ†å¤„ç†æ—¶å‡ºé”™: {e}")
    
    # æ¸…ç†èµ„æº
    del processor
    clean_memory(force_gc=True)
    
    return results

# ä¿®æ”¹çš„æ•°æ®è·å–å‡½æ•°ï¼Œæ”¯æŒåˆ†åŒº
def fetch_partitioned_data(conn, partition_id, total_partitions, limit=500000, batch_size=20000) -> Generator[List[Tuple[str, str]], None, None]:
    """åˆ†åŒºè·å–æµ‹è¯•æ•°æ®"""
    try:
        with conn.cursor() as cur:
            # åˆ›å»ºä¸´æ—¶è¡¨è¿›è¡Œåˆ†åŒº
            cur.execute("""
            CREATE TEMPORARY TABLE temp_partitioned_event_texts AS
            SELECT id as report_number, text,
                   ROW_NUMBER() OVER (ORDER BY id) as row_num
            FROM device.event_texts 
            WHERE text IS NOT NULL 
            LIMIT %s
            """, (limit,))
            
            # è·å–æ€»è¡Œæ•°
            cur.execute("SELECT COUNT(*) FROM temp_partitioned_event_texts")
            total_rows = cur.fetchone()[0]
            
            # è®¡ç®—æ¯ä¸ªåˆ†åŒºçš„å¤§å°
            partition_size = total_rows // total_partitions
            if total_rows % total_partitions > 0:
                partition_size += 1
            
            # è®¡ç®—å½“å‰åˆ†åŒºçš„å¼€å§‹å’Œç»“æŸ
            start_row = (partition_id - 1) * partition_size + 1
            end_row = min(partition_id * partition_size, total_rows)
            
            print(f"åˆ†åŒº {partition_id}/{total_partitions}: å¤„ç†è¡Œ {start_row} åˆ° {end_row} (å…± {end_row-start_row+1} è¡Œ)")
            
            # æŸ¥è¯¢å½“å‰åˆ†åŒºçš„æ•°æ®
            cur.execute("""
            SELECT report_number, text
            FROM temp_partitioned_event_texts
            WHERE row_num BETWEEN %s AND %s
            ORDER BY row_num
            """, (start_row, end_row))
            
            while True:
                records = cur.fetchmany(batch_size)
                if not records:
                    break
                yield records
                
            # åˆ é™¤ä¸´æ—¶è¡¨
            cur.execute("DROP TABLE temp_partitioned_event_texts")
            
    except Exception as e:
        logger.error(f"è·å–åˆ†åŒºæ•°æ®æ—¶å‡ºé”™: {e}")
        yield []

# å·¥ä½œèŠ‚ç‚¹åµŒå…¥å¤„ç†å‡½æ•°
def worker_embedding_processor(chunk_queue, result_queue, chunk_done, embedding_done, db_config, device, batch_size=1024):
    """å·¥ä½œèŠ‚ç‚¹çš„åµŒå…¥å¤„ç†å‡½æ•°ï¼Œä¸åŸæ¥çš„ä¼˜åŒ–åµŒå…¥å¤„ç†å™¨åŸºæœ¬ç›¸åŒ"""
    # åˆ›å»ºå¤„ç†å™¨å’Œç¼“å­˜
    embed_processor = PgVectorProcessor(db_config)
    
    # è·Ÿè¸ªç»Ÿè®¡ä¿¡æ¯
    processed_chunks = 0
    cache_hits = 0
    start_time = time.time()
    
    # çŠ¶æ€æŠ¥å‘Šé¢‘ç‡
    status_interval = 5000
    
    # åˆ›å»ºæ‰¹æ¬¡å®¹å™¨
    batch_texts = []
    batch_data = []
    
    # å…è®¸æ›´å¤§çš„æ‰¹å¤„ç†ç´¯ç§¯
    max_batch_wait = 0.1  # ç§’
    last_process_time = time.time()
    
    # å†…å­˜ä½¿ç”¨ç›‘æ§
    last_memory_check = time.time()
    memory_check_interval = 30  # ç§’
    
    # åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°
    current_batch_size = batch_size
    processing_times = []  # è·Ÿè¸ªå¤„ç†æ—¶é—´
    
    while not (chunk_done.is_set() and chunk_queue.empty()):
        try:
            # è½®è¯¢ï¼Œè¶…æ—¶ä»¥æ£€æŸ¥å®Œæˆæ ‡å¿—
            try:
                chunk = chunk_queue.get(timeout=0.05)
                report_number, chunk_idx, text = chunk
                
                # 1. ç¼“å­˜æŸ¥æ‰¾
                cached_embedding = embed_processor.embedding_cache.get(text)
                if cached_embedding is not None:
                    # ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥ä½¿ç”¨
                    chunk_id = f"{report_number}_{chunk_idx}"
                    result_queue.put((report_number, chunk_id, text, cached_embedding))
                    cache_hits += 1
                    processed_chunks += 1
                    continue
                
                # 2. æ·»åŠ åˆ°æ‰¹å¤„ç†
                batch_texts.append(text)
                batch_data.append((report_number, chunk_idx, text))
            except queue.Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œä½†ä¸æ˜¯ç»“æŸ - å¤„ç†ä»»ä½•å·²ç´¯ç§¯çš„æ‰¹æ¬¡
                pass
            
            # å†³å®šæ˜¯å¦å¤„ç†å½“å‰æ‰¹æ¬¡ (æ»¡è¶³ä»»ä¸€æ¡ä»¶):
            # 1. æ‰¹æ¬¡å¤§å°è¾¾åˆ°ç›®æ ‡
            # 2. è‡ªä¸Šæ¬¡å¤„ç†ä»¥æ¥å·²ç»è¿‡å»max_batch_waitç§’
            # 3. å·²ç»æ”¶åˆ°å®Œæˆä¿¡å·å¹¶ä¸”é˜Ÿåˆ—å‡ ä¹ä¸ºç©º
            current_time = time.time()
            should_process = (
                len(batch_texts) >= current_batch_size or
                (len(batch_texts) > 0 and current_time - last_process_time >= max_batch_wait) or
                (len(batch_texts) > 0 and chunk_done.is_set() and chunk_queue.qsize() < 10)
            )
            
            if should_process:
                # å¤„ç†æ—¶é—´æµ‹é‡
                process_start = time.time()
                
                # å¤„ç†æ‰¹æ¬¡
                if batch_texts:
                    # ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹å¤„ç†
                    embeddings = embed_processor.get_embeddings_batch(batch_texts)
                    
                    # å°†ç»“æœå‘é€åˆ°é˜Ÿåˆ—å¹¶æ›´æ–°ç¼“å­˜
                    for i, (r_num, c_idx, txt) in enumerate(batch_data):
                        if i < len(embeddings):
                            chunk_id = f"{r_num}_{c_idx}"
                            result_queue.put((r_num, chunk_id, txt, embeddings[i]))
                    
                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    processed_chunks += len(batch_texts)
                    
                    # æ¸…ç©ºæ‰¹æ¬¡
                    batch_texts = []
                    batch_data = []
                    
                    # æ›´æ–°å¤„ç†æ—¶é—´
                    process_time = time.time() - process_start
                    processing_times.append(process_time)
                    
                    # æ›´æ–°ä¸Šæ¬¡å¤„ç†æ—¶é—´
                    last_process_time = time.time()
                    
                    # åŠ¨æ€æ‰¹å¤„ç†å¤§å°è°ƒæ•´ (ä¿ç•™æœ€æ–°çš„5æ¬¡å¤„ç†)
                    if len(processing_times) > 5:
                        processing_times = processing_times[-5:]
                    
                    # å¦‚æœæœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œè°ƒæ•´æ‰¹å¤„ç†å¤§å°
                    if len(processing_times) >= 3:
                        avg_time = sum(processing_times) / len(processing_times)
                        
                        # æ ¹æ®å¹³å‡å¤„ç†æ—¶é—´è°ƒæ•´æ‰¹å¤„ç†å¤§å°
                        if avg_time < 0.5 and current_batch_size < 2048:
                            # å¤„ç†é€Ÿåº¦å¿«ï¼Œå¢åŠ æ‰¹å¤„ç†å¤§å°
                            current_batch_size = min(current_batch_size * 1.2, 2048)
                            current_batch_size = int(current_batch_size)
                        elif avg_time > 2.0 and current_batch_size > 512:
                            # å¤„ç†é€Ÿåº¦æ…¢ï¼Œå‡å°æ‰¹å¤„ç†å¤§å°
                            current_batch_size = max(current_batch_size * 0.8, 512)
                            current_batch_size = int(current_batch_size)
                
                # å‘¨æœŸæ€§åœ°æ¸…ç†å†…å­˜
                if processed_chunks % 5000 == 0 and processed_chunks > 0:
                    # è·å–å½“å‰å†…å­˜ä½¿ç”¨çŠ¶æ€
                    if time.time() - last_memory_check >= memory_check_interval:
                        import psutil
                        process = psutil.Process(os.getpid())
                        memory_mb = process.memory_info().rss / (1024 * 1024)
                        
                        print(f"  â†’ è¿›ç¨‹å†…å­˜ä½¿ç”¨: {memory_mb:.1f}MB, æ‰¹å¤„ç†å¤§å°: {current_batch_size}")
                        last_memory_check = time.time()
                        
                        # å¦‚æœå†…å­˜å ç”¨è¿‡é«˜ï¼Œæ¸…ç†
                        if memory_mb > 4000:  # 4GB
                            clean_memory(force_gc=True)
                    else:
                        clean_memory(force_gc=False)
            
            # æŠ¥å‘Šè¿›åº¦
            if processed_chunks > 0 and processed_chunks % status_interval == 0:
                elapsed = time.time() - start_time
                chunks_per_sec = processed_chunks / elapsed
                
                # ç¼“å­˜ç»Ÿè®¡
                cache_stats = embed_processor.embedding_cache.stats()
                
                print(f"  â†’ å·²å¤„ç† {processed_chunks} ä¸ªåµŒå…¥å‘é‡ "
                      f"({chunks_per_sec:.1f} ä¸ª/ç§’), "
                      f"é˜Ÿåˆ—ä¸­è¿˜æœ‰ {chunk_queue.qsize()} ä¸ªæ–‡æœ¬å—")
                print(f"  â†’ ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']*100:.1f}%, "
                      f"å‘½ä¸­/æœªå‘½ä¸­: {cache_stats['hits']}/{cache_stats['misses']}, "
                      f"æ‰¹å¤„ç†å¤§å°: {current_batch_size}")
                
        except Exception as e:
            print(f"åµŒå…¥å¤„ç†é”™è¯¯: {e}")
            # é”™è¯¯åçŸ­æš‚æš‚åœï¼Œé¿å…å¿«é€Ÿå¤±è´¥å¾ªç¯
            time.sleep(0.5)
            continue
    
    # å¤„ç†å‰©ä½™çš„æ‰¹æ¬¡
    if batch_texts:
        embeddings = embed_processor.get_embeddings_batch(batch_texts)
        for i, (r_num, c_idx, txt) in enumerate(batch_data):
            if i < len(embeddings):
                chunk_id = f"{r_num}_{c_idx}"
                result_queue.put((r_num, chunk_id, txt, embeddings[i]))
    
    # è®°å½•æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    elapsed = time.time() - start_time
    if processed_chunks > 0:
        chunks_per_sec = processed_chunks / elapsed
        cache_stats = embed_processor.embedding_cache.stats()
        
        print(f"åµŒå…¥å¤„ç†å®Œæˆ:")
        print(f"  â†’ æ€»å¤„ç†é‡: {processed_chunks} ä¸ªåµŒå…¥å‘é‡")
        print(f"  â†’ å¤„ç†é€Ÿåº¦: {chunks_per_sec:.1f} ä¸ª/ç§’")
        print(f"  â†’ æ€»è€—æ—¶: {elapsed:.1f} ç§’")
        print(f"  â†’ ç¼“å­˜å‘½ä¸­ç‡: {cache_stats['hit_rate']*100:.1f}%")
        print(f"  â†’ ä½¿ç”¨çš„è®¾å¤‡: {device}")
    
    # æ ‡è®°å®Œæˆ
    embedding_done.set()
    
    # æ¸…ç†
    del embed_processor
    clean_memory(force_gc=True)

# åè°ƒå™¨èŠ‚ç‚¹ä¸»å‡½æ•°
def run_coordinator(args):
    """åè°ƒå™¨èŠ‚ç‚¹ä¸»å‡½æ•°"""
    print("=" * 80)
    print(f"å¯åŠ¨åè°ƒå™¨èŠ‚ç‚¹ï¼Œç›‘å¬åœ°å€: {args.host}:{args.coordinator_port}")
    print("=" * 80)
    
    # åº”ç”¨M4ä¼˜åŒ–
    optimize_system_for_m4()
    
    # åˆ›å»ºåˆ†å¸ƒå¼é€šä¿¡å¯¹è±¡
    comm = DistributedComm(
        role=COORDINATOR,
        coordinator_ip=args.host,
        coordinator_port=args.coordinator_port,
        worker_port=args.worker_port
    )
    
    # æ•°æ®åº“é…ç½®
    db_config = {
        'host': args.db_host,
        'port': args.db_port,
        'dbname': args.db_name,
        'user': args.db_user,
        'password': args.db_password,
        'options': '-c search_path=device'
    }
    
    processor = PgVectorProcessor(db_config)
    
    try:
        # è¿æ¥æ•°æ®åº“
        if not processor.connect():
            logger.error("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ï¼Œç¨‹åºé€€å‡º")
            return
            
        # è®¾ç½®å‘é‡è¡¨
        table_name = args.table_name
        if not processor.setup_vector_db(table_name):
            logger.error("æ— æ³•è®¾ç½®å‘é‡æ•°æ®åº“ï¼Œç¨‹åºé€€å‡º")
            return

        print("=" * 80)
        print("å¼€å§‹åˆ†å¸ƒå¼æ•°æ®å¤„ç†...")
        print("=" * 80)
        start_time = time.time()
        
        # è·å–ä¼˜åŒ–é…ç½®
        pipeline_config = optimize_pipeline_concurrency()
        num_chunk_processes = pipeline_config['num_chunk_processes']
        num_embedding_processes = 1  # åœ¨åè°ƒå™¨èŠ‚ç‚¹åªè¿è¡Œ1ä¸ªåµŒå…¥è¿›ç¨‹
        chunk_queue_size = pipeline_config['chunk_queue_size']
        result_queue_size = pipeline_config['result_queue_size']
        embedding_batch_size = pipeline_config['embedding_batch_size']
        
        # åˆ›å»ºé˜Ÿåˆ—ç”¨äºæµæ°´çº¿
        chunk_queue = queue.Queue(maxsize=chunk_queue_size)  # å—åˆ›å»ºå’ŒåµŒå…¥ä¹‹é—´çš„ç¼“å†²åŒº
        result_queue = queue.Queue(maxsize=result_queue_size)  # ç”¨äºæ•°æ®åº“å­˜å‚¨çš„ç¼“å†²åŒº
        
        # ç”¨äºè¡¨ç¤ºå®Œæˆçš„æ ‡å¿—
        chunk_done = Event()
        embedding_done = Event()
        
        # ç­‰å¾…å·¥ä½œèŠ‚ç‚¹è¿æ¥
        print("ç­‰å¾…å·¥ä½œèŠ‚ç‚¹è¿æ¥...(10ç§’è¶…æ—¶)")
        worker_info = None
        start_wait = time.time()
        while time.time() - start_wait < 10:
            worker_info = comm.receive_message('workers', timeout=1000)
            if worker_info:
                print(f"å·¥ä½œèŠ‚ç‚¹å·²è¿æ¥: {worker_info}")
                break
            print(".", end="", flush=True)
            time.sleep(1)
        
        if not worker_info:
            print("\næ²¡æœ‰å·¥ä½œèŠ‚ç‚¹è¿æ¥ï¼Œå°†ä»¥å•æœºæ¨¡å¼è¿è¡Œ")
            # ç»§ç»­æ‰§è¡Œå•æœºæ¨¡å¼...
        else:
            print(f"\n{len(worker_info) if isinstance(worker_info, list) else 1}ä¸ªå·¥ä½œèŠ‚ç‚¹å·²è¿æ¥")
        
        # è·å–æ€»æ•°æ®é‡
        with processor.conn.cursor() as cur:
            cur.execute("""
                SELECT COUNT(*) 
                FROM (
                    SELECT 1 
                    FROM device.event_texts 
                    WHERE text IS NOT NULL 
                    LIMIT %s
                ) t
            """, (args.limit,))
            total_count = cur.fetchone()[0]
        print(f"æ€»å…±éœ€è¦å¤„ç† {total_count} æ¡è®°å½•\n")
        
        # Producer: åœ¨å•ç‹¬çš„çº¿ç¨‹æ± ä¸­åˆ‡åˆ†æ–‡æœ¬
        def chunk_producer():
            total_records = 0
            total_chunks = 0
            
            # ç¡®å®šåˆ†åŒºï¼Œåè°ƒå™¨å¤„ç†ç¬¬ä¸€ä¸ªåˆ†åŒº
            partition_id = 1
            total_partitions = 2  # å‡è®¾åªæœ‰ä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹
            
            with ProcessPoolExecutor(max_workers=num_chunk_processes) as chunk_pool:
                for batch_idx, batch in enumerate(fetch_partitioned_data(
                    processor.conn, partition_id, total_partitions, limit=args.limit, batch_size=20000)):
                    if not batch:
                        continue
                    
                    batch_start = time.time()
                    batch_size = len(batch)
                    total_records += batch_size
                    
                    print(f"æ‰¹æ¬¡ {batch_idx+1}: å¤„ç† {batch_size} æ¡è®°å½•")
                    
                    # å¹¶è¡Œå¤„ç†å—
                    futures = []
                    sub_batch_size = max(1, batch_size // num_chunk_processes)
                    sub_batches = [batch[i:i + sub_batch_size] for i in range(0, batch_size, sub_batch_size)]
                    
                    for sub_batch in sub_batches:
                        future = chunk_pool.submit(chunk_process_worker, (sub_batch, db_config))
                        futures.append(future)
                    
                    # å¤„ç†å®Œæˆçš„ç»“æœï¼ˆä¸ç­‰å¾…å…¨éƒ¨å®Œæˆï¼‰
                    for future in futures:
                        chunks = future.result()
                        total_chunks += len(chunks)
                        
                        # å°†å—æ·»åŠ åˆ°é˜Ÿåˆ—
                        for chunk in chunks:
                            chunk_queue.put(chunk)
                    
                    # è·Ÿè¸ªè¿›åº¦
                    chunk_time = time.time() - batch_start
                    print(f"  â†’ æ‰¹æ¬¡ {batch_idx+1} æ–‡æœ¬åˆ‡åˆ†å®Œæˆï¼Œè€—æ—¶ {chunk_time:.2f}ç§’")
                    
                    # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
                    elapsed = time.time() - start_time
                    partition_progress = total_records / (total_count / total_partitions)
                    print(f"\nåˆ†åŒºè¿›åº¦: å·²å¤„ç† {total_records} æ¡è®°å½• ({partition_progress*100:.1f}%)")
                    if total_records > 0:
                        est_total = elapsed / partition_progress
                        remaining = est_total - elapsed
                        print(f"  â†’ å·²ç”¨æ—¶é—´: {elapsed/60:.1f}åˆ†é’Ÿï¼Œé¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ")
                    print(f"  â†’ å·²ç”Ÿæˆ {total_chunks} ä¸ªæ–‡æœ¬å—ï¼Œé˜Ÿåˆ—å¤§å°: {chunk_queue.qsize()}")
                
                # æ ‡è®°å®Œæˆ
                chunk_done.set()
                print("æ‰€æœ‰æ–‡æœ¬åˆ‡åˆ†ä»»åŠ¡å®Œæˆ")
        
        # Consumer: å°†ç»“æœé«˜æ•ˆå­˜å‚¨åœ¨æ•°æ®åº“ä¸­
        def db_consumer():
            # ç”¨äºç»Ÿè®¡çš„è®¡æ•°å™¨
            stored_count = 0
            batch = []
            
            while not (embedding_done.is_set() and result_queue.empty()):
                try:
                    # æ”¶é›†æ‰¹æ¬¡è¿›è¡Œæ•°æ®åº“æ’å…¥
                    while len(batch) < 5000:
                        try:
                            item = result_queue.get(timeout=0.1)
                            batch.append(item)
                            
                            # å¦‚æœåœ¨æ¸…ç©ºæœ€åçš„é¡¹ç›®ï¼Œä¸­æ–­
                            if embedding_done.is_set() and result_queue.empty():
                                break
                        except queue.Empty:
                            # é˜Ÿåˆ—æš‚æ—¶ä¸ºç©ºä½†è¿˜æœªç»“æŸ
                            break
                    
                    # å­˜å‚¨æ‰¹æ¬¡
                    if batch:
                        store_start = time.time()
                        if processor.store_vectors(table_name, batch):
                            stored_count += len(batch)
                            print(f"  â†’ æˆåŠŸå­˜å‚¨ {len(batch)} ä¸ªå‘é‡ï¼Œæ€»è®¡: {stored_count}ï¼Œè€—æ—¶ {time.time() - store_start:.2f}ç§’")
                        batch = []
                except Exception as e:
                    print(f"å­˜å‚¨å‘é‡æ—¶å‡ºé”™: {e}")
                    time.sleep(0.1)
                    continue
            
            # å­˜å‚¨å‰©ä½™æ‰¹æ¬¡
            if batch:
                if processor.store_vectors(table_name, batch):
                    stored_count += len(batch)
                    print(f"  â†’ æˆåŠŸå­˜å‚¨æœ€å {len(batch)} ä¸ªå‘é‡ï¼Œæ€»è®¡: {stored_count}")
            
            print(f"æ‰€æœ‰å‘é‡å­˜å‚¨ä»»åŠ¡å®Œæˆï¼Œå…±å­˜å‚¨ {stored_count} ä¸ªå‘é‡")
        
        # å¯åŠ¨æµæ°´çº¿çº¿ç¨‹
        try:
            print("å¯åŠ¨åˆ†å¸ƒå¼æµæ°´çº¿å¤„ç†...")
            # åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
            chunk_thread = Thread(target=chunk_producer)
            db_thread = Thread(target=db_consumer)
            
            chunk_thread.start()
            # å°å»¶è¿Ÿè®©å—å¼€å§‹ç§¯ç´¯
            time.sleep(1)
            
            # åˆ›å»ºåµŒå…¥å¤„ç†çº¿ç¨‹(æœ¬åœ°)
            embedding_thread = Thread(
                target=worker_embedding_processor,
                args=(chunk_queue, result_queue, chunk_done, embedding_done, db_config, device, embedding_batch_size)
            )
            embedding_thread.start()
            
            # å°å»¶è¿Ÿè®©åµŒå…¥å¼€å§‹ç§¯ç´¯
            time.sleep(1)
            db_thread.start()
            
            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
            chunk_thread.join()
            embedding_thread.join()
            db_thread.join()
            
            # ç»Ÿè®¡ä¿¡æ¯è¾“å‡º
            end_time = time.time()
            processing_time = end_time - start_time
            print("\nå¤„ç†å®Œæˆ:")
            print("=" * 80)
            print(f"æ€»è€—æ—¶: {processing_time:.2f} ç§’ ({processing_time/60:.2f} åˆ†é’Ÿ)")
            print(f"ä½¿ç”¨çš„è®¾å¤‡: {device.upper()}")
            print(f"æ–‡æœ¬åˆ‡åˆ†è¿›ç¨‹æ•°: {num_chunk_processes}")
            print(f"åµŒå…¥è®¡ç®—è¿›ç¨‹æ•°: {num_embedding_processes}")
            print(f"æ‰¹å¤„ç†å¤§å°: {embedding_batch_size}")
            print("=" * 80)
            
        except Exception as e:
            logger.error(f"æµæ°´çº¿å¤„ç†å‡ºé”™: {e}")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    finally:
        processor.close()
        comm.close()

# å·¥ä½œèŠ‚ç‚¹ä¸»å‡½æ•°
def run_worker(args):
    """å·¥ä½œèŠ‚ç‚¹ä¸»å‡½æ•°"""
    print("=" * 80)
    print(f"å¯åŠ¨å·¥ä½œèŠ‚ç‚¹ï¼Œè¿æ¥åˆ°åè°ƒå™¨: {args.coordinator_host}:{args.coordinator_port}")
    print("=" * 80)
    
    # åº”ç”¨M4ä¼˜åŒ–
    optimize_system_for_m4()
    
    # åˆ›å»ºåˆ†å¸ƒå¼é€šä¿¡å¯¹è±¡
    comm = DistributedComm(
        role=WORKER,
        coordinator_ip=args.coordinator_host,
        coordinator_port=args.coordinator_port,
        worker_port=args.worker_port
    )
    
    # æ•°æ®åº“é…ç½®
    db_config = {
        'host': args.db_host,
        'port': args.db_port,
        'dbname': args.db_name,
        'user': args.db_user,
        'password': args.db_password,
        'options': '-c search_path=device'
    }
    
    processor = PgVectorProcessor(db_config)
    
    try:
        # è¿æ¥æ•°æ®åº“
        if not processor.connect():
            logger.error("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ï¼Œç¨‹åºé€€å‡º")
            return
        
        # å‘åè°ƒå™¨æ³¨å†Œ
        worker_info = {
            'hostname': socket.gethostname(),
            'pid': os.getpid(),
            'device': device,
            'cores': os.cpu_count(),
            'timestamp': time.time()
        }
        comm.send_message('coordinator', worker_info)
        
        print("å‘åè°ƒå™¨æ³¨å†ŒæˆåŠŸï¼Œç­‰å¾…ä»»åŠ¡åˆ†é…...")
        
        # è·å–ä¼˜åŒ–é…ç½®
        pipeline_config = optimize_pipeline_concurrency()
        num_chunk_processes = pipeline_config['num_chunk_processes']
        num_embedding_processes = pipeline_config['num_embedding_processes']
        chunk_queue_size = pipeline_config['chunk_queue_size']
        result_queue_size = pipeline_config['result_queue_size']
        embedding_batch_size = pipeline_config['embedding_batch_size']
        
        # åˆ›å»ºé˜Ÿåˆ—ç”¨äºæµæ°´çº¿
        chunk_queue = queue.Queue(maxsize=chunk_queue_size)
        result_queue = queue.Queue(maxsize=result_queue_size)
        
        # ç”¨äºè¡¨ç¤ºå®Œæˆçš„æ ‡å¿—
        chunk_done = Event()
        embedding_done = Event()
        
        # å·¥ä½œèŠ‚ç‚¹å¤„ç†ç¬¬äºŒä¸ªåˆ†åŒº
        partition_id = 2
        total_partitions = 2
        
        # Producer: åœ¨å•ç‹¬çš„çº¿ç¨‹æ± ä¸­åˆ‡åˆ†æ–‡æœ¬
        def chunk_producer():
            total_records = 0
            total_chunks = 0
            
            with ProcessPoolExecutor(max_workers=num_chunk_processes) as chunk_pool:
                for batch_idx, batch in enumerate(fetch_partitioned_data(
                    processor.conn, partition_id, total_partitions, limit=args.limit, batch_size=20000)):
                    if not batch:
                        continue
                    
                    batch_start = time.time()
                    batch_size = len(batch)
                    total_records += batch_size
                    
                    print(f"æ‰¹æ¬¡ {batch_idx+1}: å¤„ç† {batch_size} æ¡è®°å½•")
                    
                    # å¹¶è¡Œå¤„ç†å—
                    futures = []
                    sub_batch_size = max(1, batch_size // num_chunk_processes)
                    sub_batches = [batch[i:i + sub_batch_size] for i in range(0, batch_size, sub_batch_size)]
                    
                    for sub_batch in sub_batches:
                        future = chunk_pool.submit(chunk_process_worker, (sub_batch, db_config))
                        futures.append(future)
                    
                    # å¤„ç†å®Œæˆçš„ç»“æœï¼ˆä¸ç­‰å¾…å…¨éƒ¨å®Œæˆï¼‰
                    for future in futures:
                        chunks = future.result()
                        total_chunks += len(chunks)
                        
                        # å°†å—æ·»åŠ åˆ°é˜Ÿåˆ—
                        for chunk in chunks:
                            chunk_queue.put(chunk)
                    
                    # è·Ÿè¸ªè¿›åº¦
                    chunk_time = time.time() - batch_start
                    print(f"  â†’ æ‰¹æ¬¡ {batch_idx+1} æ–‡æœ¬åˆ‡åˆ†å®Œæˆï¼Œè€—æ—¶ {chunk_time:.2f}ç§’")
                    print(f"  â†’ å·²ç”Ÿæˆ {total_chunks} ä¸ªæ–‡æœ¬å—ï¼Œé˜Ÿåˆ—å¤§å°: {chunk_queue.qsize()}")
                
                # æ ‡è®°å®Œæˆ
                chunk_done.set()
                print("æ‰€æœ‰æ–‡æœ¬åˆ‡åˆ†ä»»åŠ¡å®Œæˆ")
        
        # å·¥ä½œèŠ‚ç‚¹æ— éœ€æ•°æ®åº“å†™å…¥ï¼Œç»“æœç›´æ¥å‘é€åˆ°åè°ƒå™¨
        def result_sender():
            sent_count = 0
            batch = []
            batch_size = 100  # å‘é€æ‰¹æ¬¡å¤§å°
            
            while not (embedding_done.is_set() and result_queue.empty()):
                try:
                    # æ”¶é›†æ‰¹æ¬¡
                    while len(batch) < batch_size:
                        try:
                            item = result_queue.get(timeout=0.1)
                            batch.append(item)
                            
                            # å¦‚æœåœ¨æ¸…ç©ºæœ€åçš„é¡¹ç›®ï¼Œä¸­æ–­
                            if embedding_done.is_set() and result_queue.empty():
                                break
                        except queue.Empty:
                            # é˜Ÿåˆ—æš‚æ—¶ä¸ºç©ºä½†è¿˜æœªç»“æŸ
                            break
                    
                    # å‘é€æ‰¹æ¬¡åˆ°åè°ƒå™¨
                    if batch:
                        # ä½¿ç”¨ZeroMQå‘é€ç»“æœ
                        if comm.send_message('tasks', batch):
                            sent_count += len(batch)
                            if sent_count % 1000 == 0:
                                print(f"  â†’ å·²å‘é€ {sent_count} ä¸ªå‘é‡ç»“æœåˆ°åè°ƒå™¨")
                        batch = []
                except Exception as e:
                    print(f"å‘é€ç»“æœæ—¶å‡ºé”™: {e}")
                    time.sleep(0.1)
                    continue
            
            # å‘é€å‰©ä½™æ‰¹æ¬¡
            if batch:
                if comm.send_message('tasks', batch):
                    sent_count += len(batch)
            
            print(f"æ‰€æœ‰ç»“æœå‘é€å®Œæˆï¼Œå…±å‘é€ {sent_count} ä¸ªå‘é‡ç»“æœ")
            
            # é€šçŸ¥åè°ƒå™¨æœ¬èŠ‚ç‚¹å¤„ç†å®Œæˆ
            comm.send_message('coordinator', {'status': 'complete', 'count': sent_count})
        
        # å¯åŠ¨æµæ°´çº¿çº¿ç¨‹
        try:
            print("å¯åŠ¨å·¥ä½œèŠ‚ç‚¹å¤„ç†æµæ°´çº¿...")
            # åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
            chunk_thread = Thread(target=chunk_producer)
            result_thread = Thread(target=result_sender)
            
            chunk_thread.start()
            # å°å»¶è¿Ÿè®©å—å¼€å§‹ç§¯ç´¯
            time.sleep(1)
            
            # åˆ›å»ºåµŒå…¥å¤„ç†çº¿ç¨‹
            embedding_threads = []
            for i in range(num_embedding_processes):
                thread = Thread(
                    target=worker_embedding_processor,
                    args=(chunk_queue, result_queue, chunk_done, embedding_done, db_config, device, embedding_batch_size)
                )
                thread.start()
                embedding_threads.append(thread)
                # ä¸ºæ¯ä¸ªåµŒå…¥çº¿ç¨‹æ·»åŠ å°å»¶è¿Ÿï¼Œé¿å…åŒæ—¶å¯åŠ¨é€ æˆçš„èµ„æºç«äº‰
                time.sleep(0.5)
            
            # å°å»¶è¿Ÿè®©åµŒå…¥å¼€å§‹ç§¯ç´¯
            time.sleep(1)
            result_thread.start()
            
            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
            chunk_thread.join()
            for thread in embedding_threads:
                thread.join()
            result_thread.join()
            
            print("å·¥ä½œèŠ‚ç‚¹æ‰€æœ‰å¤„ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"å·¥ä½œèŠ‚ç‚¹å¤„ç†å‡ºé”™: {e}")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    finally:
        processor.close()
        comm.close()

def main():
    parser = argparse.ArgumentParser(description='åˆ†å¸ƒå¼å‘é‡åµŒå…¥å¤„ç†ç³»ç»Ÿ')
    parser.add_argument('--role', choices=[COORDINATOR, WORKER], required=True, help='èŠ‚ç‚¹è§’è‰² (coordinator æˆ– worker)')
    
    # æ•°æ®åº“å‚æ•°
    parser.add_argument('--db-host', default='169.254.22.165', help='æ•°æ®åº“ä¸»æœºåœ°å€')
    parser.add_argument('--db-port', type=int, default=5432, help='æ•°æ®åº“ç«¯å£')
    parser.add_argument('--db-name', default='fda_device', help='æ•°æ®åº“åç§°')
    parser.add_argument('--db-user', default='postgres', help='æ•°æ®åº“ç”¨æˆ·å')
    parser.add_argument('--db-password', default='12345687', help='æ•°æ®åº“å¯†ç ')
    parser.add_argument('--table-name', default='event_text_vectors', help='å‘é‡è¡¨åç§°')
    
    # åˆ†å¸ƒå¼é€šä¿¡å‚æ•°
    parser.add_argument('--host', default='0.0.0.0', help='æœ¬æœºIPï¼Œåè°ƒå™¨èŠ‚ç‚¹ä½¿ç”¨')
    parser.add_argument('--coordinator-host', default='localhost', help='åè°ƒå™¨IPï¼Œå·¥ä½œèŠ‚ç‚¹ä½¿ç”¨')
    parser.add_argument('--coordinator-port', type=int, default=5555, help='åè°ƒå™¨ç«¯å£')
    parser.add_argument('--worker-port', type=int, default=5556, help='å·¥ä½œèŠ‚ç‚¹é€šä¿¡ç«¯å£')
    
    # ä»»åŠ¡å‚æ•°
    parser.add_argument('--limit', type=int, default=500000, help='å¤„ç†çš„æœ€å¤§è®°å½•æ•°')
    
    args = parser.parse_args()
    
    if args.role == COORDINATOR:
        run_coordinator(args)
    else:
        run_worker(args)

if __name__ == "__main__":
    main()